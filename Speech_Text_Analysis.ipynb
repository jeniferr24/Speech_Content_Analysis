{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFQQNTudz9rh",
        "outputId": "f80e653b-85ab-4155-c635-651410683464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/125.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=9b407475bf53fad2f912a6393d96f64e36c7f4649aa5120c9b526ba42a161747\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNs3fSxs25bL"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from num2words import num2words\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import copy\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "f-qEd0uL0iAS",
        "outputId": "23d1722b-835d-4025-f531-c5d72fe64d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-191d4e21-ff93-48ea-a6f1-ebcdc5fb5e4b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-191d4e21-ff93-48ea-a6f1-ebcdc5fb5e4b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving stories.zip to stories.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the uploaded ZIP archive filename\n",
        "zip_filename = 'stories.zip'\n",
        "\n",
        "# Extract the contents of the ZIP archive\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n"
      ],
      "metadata": {
        "id": "mOK_XyB-2xnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"stories\"\n",
        "alpha = 0.3"
      ],
      "metadata": {
        "id": "dm_-BmYe0FBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders = [x[0] for x in os.walk(str(os.getcwd())+'/'+title+'/')]\n",
        "folders[0] = folders[0][:len(folders[0])-1]"
      ],
      "metadata": {
        "id": "RS8Ucj660JkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP_cOREj3Hpl",
        "outputId": "fdd4eeb9-f559-4ea4-d665-84547223c5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/stories', '/content/stories/SRE', '/content/stories/FARNON']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  The main purpose of this code appears to be to create a dataset of file paths and titles by extracting information from HTML files within\n",
        "# a set of folders."
      ],
      "metadata": {
        "id": "FXQvAbDP7dD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "c = False\n",
        "\n",
        "for i in folders:\n",
        "    file = open(i+\"/index.html\", 'r')\n",
        "    text = file.read().strip()\n",
        "    file.close()\n",
        "\n",
        "    file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
        "    file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
        "\n",
        "    if c == False:\n",
        "        file_name = file_name[2:]\n",
        "        c = True\n",
        "\n",
        "    print(len(file_name), len(file_title))\n",
        "\n",
        "    for j in range(len(file_name)):\n",
        "        dataset.append((str(i) +\"/\"+ str(file_name[j]), file_title[j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-98NsGHi3MOm",
        "outputId": "f404295c-871a-49cf-85b8-2e70492a685c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "452 452\n",
            "15 15\n",
            "0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 452 452: During the first iteration, the file_name and file_title lists both have 452 elements each. This suggests that the regular expressions used to extract data from the HTML content successfully found 452 matches for both file names and file titles in the first folder's index.html file.\n",
        "\n",
        "# 15 15: During the second iteration, the file_name and file_title lists both have 15 elements each. This indicates that in the second folder's index.html file, there were 15 matches for both file names and file titles.\n",
        "\n",
        "# 0 0: The third line of output indicates that during the third iteration, both file_name and file_title lists have 0 elements each. This suggests that either the regular expressions didn't match any content in the third folder's index.html file, or there might have been some issue with reading the file or the regular expressions themselves."
      ],
      "metadata": {
        "id": "qL8uuvdd8PMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzcLtqHe3WcO",
        "outputId": "995324a1-073d-4fc9-dd97-e8e2b1a4cd9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = len (dataset)"
      ],
      "metadata": {
        "id": "3kbg7tyi3YZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_doc(id):\n",
        "    print(dataset[id])\n",
        "    file = open(dataset[id][0], 'r', encoding='cp1250')\n",
        "    text = file.read().strip()\n",
        "    file.close()\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "gAiDs-BK3YSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_lower_case(data):\n",
        "    return np.char.lower(data)"
      ],
      "metadata": {
        "id": "xOGYhhij3fhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(data):\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in words:\n",
        "        if w not in stop_words and len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "aVGCGsnv3jHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        data = np.char.replace(data, symbols[i], ' ')\n",
        "        data = np.char.replace(data, \"  \", \" \")\n",
        "    data = np.char.replace(data, ',', '')\n",
        "    return data"
      ],
      "metadata": {
        "id": "mR5jZtS03mZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ],
      "metadata": {
        "id": "gGK-HYTr3q2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(data):\n",
        "    stemmer= PorterStemmer()\n",
        "\n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        new_text = new_text + \" \" + stemmer.stem(w)\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "1Mg1RAcy3t2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_numbers(data):\n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            w = num2words(int(w))\n",
        "        except:\n",
        "            a = 0\n",
        "        new_text = new_text + \" \" + w\n",
        "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "08J1hoZE3xzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data) #remove comma seperately\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_stop_words(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = stemming(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = stemming(data) #needed again as we need to stem the words\n",
        "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
        "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
        "    return data"
      ],
      "metadata": {
        "id": "HYH8fdqs31wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0szo6fuf3_Bv",
        "outputId": "db197a40-bf30-47f9-8ae3-e82beeed5628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text = []\n",
        "processed_title = []\n",
        "\n",
        "for i in dataset[:N]:\n",
        "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
        "    text = file.read().strip()\n",
        "    file.close()\n",
        "\n",
        "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
        "    processed_title.append(word_tokenize(str(preprocess(i[1]))))"
      ],
      "metadata": {
        "id": "dCQuoJnB35pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DF = {}\n",
        "\n",
        "for i in range(N):\n",
        "    tokens = processed_text[i]\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            DF[w].add(i)\n",
        "        except:\n",
        "            DF[w] = {i}\n",
        "\n",
        "    tokens = processed_title[i]\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            DF[w].add(i)\n",
        "        except:\n",
        "            DF[w] = {i}\n",
        "for i in DF:\n",
        "    DF[i] = len(DF[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "6tStFKi24lXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_vocab_size = len(DF)"
      ],
      "metadata": {
        "id": "VE_CPZvv45hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5odZ5oH646Vt",
        "outputId": "1dd03cdd-2919-437a-d9c6-341683ad2de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32350"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_vocab = [x for x in DF]"
      ],
      "metadata": {
        "id": "qyQr05d94o_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_vocab[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJoEw9Y25F7_",
        "outputId": "274a8b1f-3037-4ad2-fad1-717562c1e60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sharewar', 'trial', 'project', 'freewar', 'need', 'support', 'continu', 'one', 'hundr', 'west', 'fifti', 'three', 'north', 'jim', 'prentic', 'copyright', 'thousand', 'nine', 'nineti', 'brandon']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_freq(word):\n",
        "    c = 0\n",
        "    try:\n",
        "        c = DF[word]\n",
        "    except:\n",
        "        pass\n",
        "    return c"
      ],
      "metadata": {
        "id": "MFr1rrfv5JhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = 0\n",
        "\n",
        "tf_idf = {}\n",
        "\n",
        "for i in range(N):\n",
        "\n",
        "    tokens = processed_text[i]\n",
        "\n",
        "    counter = Counter(tokens + processed_title[i])\n",
        "    words_count = len(tokens + processed_title[i])\n",
        "\n",
        "    for token in np.unique(tokens):\n",
        "\n",
        "        tf = counter[token]/words_count\n",
        "        df = doc_freq(token)\n",
        "        idf = np.log((N+1)/(df+1))\n",
        "\n",
        "        tf_idf[doc, token] = tf*idf\n",
        "\n",
        "    doc += 1"
      ],
      "metadata": {
        "id": "tL8v-zXJ5NWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = 0\n",
        "\n",
        "tf_idf_title = {}\n",
        "\n",
        "for i in range(N):\n",
        "\n",
        "    tokens = processed_title[i]\n",
        "    counter = Counter(tokens + processed_text[i])\n",
        "    words_count = len(tokens + processed_text[i])\n",
        "\n",
        "    for token in np.unique(tokens):\n",
        "\n",
        "        tf = counter[token]/words_count\n",
        "        df = doc_freq(token)\n",
        "        idf = np.log((N+1)/(df+1)) #numerator is added 1 to avoid negative values\n",
        "\n",
        "        tf_idf_title[doc, token] = tf*idf\n",
        "\n",
        "    doc += 1"
      ],
      "metadata": {
        "id": "qvpnWzL05Q7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf[(0,\"go\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXtw78kn5Uud",
        "outputId": "805cf578-bdb8-4838-c407-55c646966a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0002906893990853149"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_title[(0,\"go\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByV8VKB05YS_",
        "outputId": "959ce59a-19c5-437d-ea63-7da2f9d1ac92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0002906893990853149"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tf_idf:\n",
        "    tf_idf[i] *= alpha\n",
        "for i in tf_idf_title:\n",
        "    tf_idf[i] = tf_idf_title[i]"
      ],
      "metadata": {
        "id": "1jGPteiQ5gBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tf_idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjlPxkWl5heH",
        "outputId": "ebea1144-5124-4168-8381-4a016fa337fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "344378"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def matching_score(k, query):\n",
        "    preprocessed_query = preprocess(query)\n",
        "    tokens = word_tokenize(str(preprocessed_query))\n",
        "\n",
        "    print(\"Matching Score\")\n",
        "    print(\"\\nQuery:\", query)\n",
        "    print(\"\")\n",
        "    print(tokens)\n",
        "\n",
        "    query_weights = {}\n",
        "\n",
        "    for key in tf_idf:\n",
        "\n",
        "        if key[1] in tokens:\n",
        "            try:\n",
        "                query_weights[key[0]] += tf_idf[key]\n",
        "            except:\n",
        "                query_weights[key[0]] = tf_idf[key]\n",
        "\n",
        "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "    l = []\n",
        "\n",
        "    for i in query_weights[:10]:\n",
        "        l.append(i[0])\n",
        "\n",
        "    print(l)\n",
        "\n",
        "\n",
        "matching_score(10, \"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYiLmCG85kt8",
        "outputId": "052b69da-9a63-4087-831f-7f85798bb447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching Score\n",
            "\n",
            "Query: Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\n",
            "\n",
            "['without', 'drive', 'rebeccah', 'insist', 'kate', 'lost', 'momentum', 'stood', 'next', 'slat', 'oak', 'bench', 'canist', 'still', 'clutch', 'survey']\n",
            "\n",
            "[166, 200, 352, 433, 211, 350, 175, 187, 188, 294]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_sim(a, b):\n",
        "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "    return cos_sim"
      ],
      "metadata": {
        "id": "QLL9y7hl5kiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = np.zeros((N, total_vocab_size))\n",
        "for i in tf_idf:\n",
        "    try:\n",
        "        ind = total_vocab.index(i[1])\n",
        "        D[i[0]][ind] = tf_idf[i]\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "0iOx3I-m5uiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_vector(tokens):\n",
        "\n",
        "    Q = np.zeros((len(total_vocab)))\n",
        "\n",
        "    counter = Counter(tokens)\n",
        "    words_count = len(tokens)\n",
        "\n",
        "    query_weights = {}\n",
        "\n",
        "    for token in np.unique(tokens):\n",
        "\n",
        "        tf = counter[token]/words_count\n",
        "        df = doc_freq(token)\n",
        "        idf = math.log((N+1)/(df+1))\n",
        "\n",
        "        try:\n",
        "            ind = total_vocab.index(token)\n",
        "            Q[ind] = tf*idf\n",
        "        except:\n",
        "            pass\n",
        "    return Q"
      ],
      "metadata": {
        "id": "kv5BhCHj59aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(k, query):\n",
        "    print(\"Cosine Similarity\")\n",
        "    preprocessed_query = preprocess(query)\n",
        "    tokens = word_tokenize(str(preprocessed_query))\n",
        "\n",
        "    print(\"\\nQuery:\", query)\n",
        "    print(\"\")\n",
        "    print(tokens)\n",
        "\n",
        "    d_cosines = []\n",
        "\n",
        "    query_vector = gen_vector(tokens)\n",
        "\n",
        "    for d in D:\n",
        "        d_cosines.append(cosine_sim(query_vector, d))\n",
        "\n",
        "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "    print(out)\n",
        "\n",
        "#     for i in out:\n",
        "#         print(i, dataset[i][0])\n",
        "\n",
        "\n",
        "Q = cosine_similarity(10, \"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucYTDdcO59Wt",
        "outputId": "6ed6abba-5bef-40a4-deeb-cc16aff9eafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity\n",
            "\n",
            "Query: Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\n",
            "\n",
            "['without', 'drive', 'rebeccah', 'insist', 'kate', 'lost', 'momentum', 'stood', 'next', 'slat', 'oak', 'bench', 'canist', 'still', 'clutch', 'survey']\n",
            "\n",
            "[200 166 433 175 169 402 211  87 151 369]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_doc(200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntv3fxU56y9Z",
        "outputId": "322269c4-01b0-47ed-f6c5-5cf35158ac54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('/content/stories/ghost', 'Time for Flowers, by Gay Bost')\n",
            "TIME FOR FLOWERS\n",
            "  by Gay Bost\n",
            "\n",
            "They'd put flowers up. She hadn't noticed. Time wouldn't hold still.\n",
            "She remembered, quite clearly, that time had been a simple thing; one\n",
            "moment following the previous one, seconds strung out neatly like her\n",
            "mother's pearls laid out on the dark mahogany vanity each Sunday\n",
            "morning. But there had been a catch . . . \n",
            "\n",
            "Hung around Mother's neck the catch clicked and the tidy little line \n",
            "of seconds became a never ending circle with only the catch in the \n",
            "middle. For some reason the thought of pearls gathered from the sea, \n",
            "naturally nested within the confines of oyster shells, scattered \n",
            "haphazardly about the ocean floor disturbed her.\n",
            "\n",
            "Now they'd put up the flowers in the same careless groupings. This,\n",
            "too, disturbed her. Bright yellow trumpets, their collars spread to\n",
            "catch the sun, dotted the front yard in clusters of two or three, five\n",
            "or six. Bunches laid carelessly and forgotten. In a moment she'd\n",
            "come away from the window and have a word with the gardener. He\n",
            "listened so well and explained to others so reasonably why this should\n",
            "be so instead of the way they wanted it done, how that would look\n",
            "better or cut the wind more effectively.\n",
            "\n",
            "And then she recalled his stiff body stretched out in the little bed\n",
            "over the garages. Another pearl had come loose from the strand,\n",
            "seeming to want to search out its old home in a far away oyster bed.\n",
            "She would have those pearls laid out neatly, one following the one\n",
            "before and so on and so on. She would have those damned yellow\n",
            "flowers marching smartly along the walk. She'd have it if she\n",
            "had to go out there and replant each and every one of them.\n",
            "\n",
            "She flew down the hallway and sailed over the steps leading the\n",
            "back way to the kitchen, much as she had done as a child. Where then\n",
            "she had skipped in joy she now catapulted her form in anger.\n",
            "\n",
            "\"And there you are!\" she said, as she encountered the woman she had \n",
            "come to know as Kate. All of five foot tall in her stocking feet and \n",
            "surely every bit of two hundred pounds, her pudgy fists more often \n",
            "than not braced on the sudden outburst of her hips. So she stood, \n",
            "having turned from the sink. Suds and water darkened the fabric of her \n",
            "dress. Her face was pleasant; round, rosy cheeked, with eyes the color \n",
            "of mint in the summer sunset. \"And *where have you been these three days*?\"\n",
            "\n",
            "\"I want the flowers straightened out,\" Rebeccah said. \"I want the\n",
            "flowers placed in the proper alignments.\"\n",
            "\n",
            "Kate tilted her head, narrowed her eyes and frowned. \"Ah, you're in a\n",
            "huff again. What can it be this time?\"\n",
            "\n",
            "\"I want the flours straightened out,\" Rebeccah yelled, coming up to\n",
            "the woman's face.\n",
            "\n",
            "Kate went directly to the cupboard, strained upon her tiny toes to\n",
            "reach the second shelf, and pulled the flour canister out. She set it\n",
            "on the counter. She repeated the process, bringing out a smaller\n",
            "canister. Rebecca knew this one to be the unbleached flour Kate used\n",
            "for one particular recipe.\n",
            "\n",
            "\"No,no, no!\"  Rebeccah hissed. \"Flowers!  Not flours!\"  She propped\n",
            "herself against the edge of the kitchen table and crossed her arms\n",
            "over her chest, waiting for the woman to get it right.\n",
            "\n",
            "Kate stood looking dumbly at the canisters. \"Now, what was I going to\n",
            "do with these?\"  she asked herself. She drummed her fingers on the\n",
            "counter top before bringing one hand to her lips, where the pointer\n",
            "finger tapped on her upper lip.\n",
            "\n",
            "\"The Flowers!  Outside!\"  Rebecca screamed, highly agitated.\n",
            "\n",
            "Kate gathered the two canisters and moved toward the back door, one\n",
            "held against her ample form by each arm.\n",
            "\n",
            "Exasperated, Rebeccah followed her out, watching to see what she would do.\n",
            "\n",
            "Without the drive of Rebeccah's insistence, Kate lost her momentum.\n",
            "She stood next a slatted oak bench, canisters still clutched, surveying \n",
            "the sunlit yard and gardens beyond. Harold had done a passable job \n",
            "trimming the hedges, but Kate missed the gardener's touch. She resolved \n",
            "to contact the nursery and find another. Flaux, bright purples, pinks \n",
            "and radiant white encircled the herb garden, a brilliant contrast to \n",
            "the varied greens within. She set the canisters down on the bench and \n",
            "moved toward the cheerful scene.\n",
            "\n",
            "Rebeccah, discouraged, sat primly on the edge of the bench, dusting a\n",
            "wisp of hair away from her temple. New mint, dew draped, veiled a\n",
            "border of stocky wooden poles to trail onto the walk, had been crushed, \n",
            "probably by the man of the house on his way off to work. The scent \n",
            "filled her nostrils. She found herself a child, again, tasting her \n",
            "first tea with mint -- fresh cut from the gardens. _\"How long has it\n",
            "been?\"_  she wondered. Kate had gone down on her knees over the flaux,\n",
            "bending to weed through the thyme.\n",
            "\n",
            "\"I don't know why I have to put up with idiots,\" Rebeccah complained.\n",
            "\"It all so worthless, so futile.\"  With a great sigh she rose from the\n",
            "bench and made her way back into the house. The bright kitchen seemed\n",
            "a waste of life, all a travesty to cover the desolation of her\n",
            "unnaturally extended existence. \n",
            "\n",
            "She faced the stairs with exhaustion, deciding, instead, to forego the \n",
            "trip up. She sat on the bottom step, delicate chin propped on tightly \n",
            "curled fists, gazing dully at the open pantry door, seeing into the past \n",
            "-- again. Where, in this world the shelves were haphazardly stacked with \n",
            "cans of peaches and corn, she saw row after row of glass jars. Beets!  \n",
            "Ugh!  Her grandmother's pickled beets, always pretty to view, left a \n",
            "phantom bitterness within her mouth.\n",
            "\n",
            "On the lawn Kate sat back on her heels, suddenly lost in sorrow and\n",
            "self-pity. Tears streamed down her cheeks to drop onto the fabric of\n",
            "her dress. She thought of Harold, busily showing homes as lovely as\n",
            "their own to strangers while she ruined her nails weeding this pitiful\n",
            "excuse for a garden. She shoved her pudgy fists into her burning eyes\n",
            "and wept aloud for the waste of her life. She sniffed back her running \n",
            "nose . . . sniffed again. She snuffled like a dog scenting something \n",
            "unusual, nose in the air. \"Beets?\"  she asked aloud. \"Beets?\"  Her \n",
            "hands dropped to her thighs, pushing to rise. _\"Of course,\"_ she thought \n",
            "to herself, _\"this *lovely* house is haunted by a very emotional woman.\"_  \n",
            "Her knees ached. She turned toward the house and noticed the flour \n",
            "canisters on the bench. \"And whatever she wants *this* time is not \n",
            "getting through this thick skull of mine!\"\n",
            "\n",
            "Kate knuckle-rapped herself above her right temple. \"Rebeccah!\"  she\n",
            "called. \"Quit moping!  You'll ruin another day for me and I still\n",
            "have to deal with that horrible Avon woman this morning.\"\n",
            "\n",
            "\"I want my flowers properly aligned!\" Rebeccah screamed from the stairs.\n",
            "\n",
            "As Kate passed the bench she paused to move the flour canisters so\n",
            "that the labels faced in the same direction, each perfectly centered\n",
            "over three of the wood slats. With a self-satisfied air she re-entered \n",
            "her own kitchen. \"Now,\" she began, addressing the refrigerator, \"what \n",
            "we need is improved communication.\"\n",
            "\n",
            "\"Fool,\" hissed Rebeccah, \"you're talking to the refrigerator again.\"\n",
            "\n",
            "\"You don't want an empath. You want a telepath,\" Kate said, turning\n",
            "to stare at Rebeccah with surprising accuracy.\n",
            "\n",
            "The two women blinked at each other and broke into laughter.\n",
            "\n",
            "\"I want my flowers straightened out!\"  Rebeccah commented softly when\n",
            "the mirth had passed.\n",
            "\n",
            "                              * * *\n",
            "\n",
            "\"There!\"  Kate replaced the telephone hand piece and pocketed the\n",
            "scrap of paper she'd written the new gardener's name upon. \"Mr.\n",
            "Hi-a-cow-wah,\" she practiced aloud. \"Very good.\"  The door chime rang\n",
            "throughout the house, echoing off the tiled kitchen walls.\n",
            "\n",
            "\"Oh, no!\"  wailed Rebeccah. \"Not Japanese!  They have such spiritual\n",
            "ideas on gardening -- I'll never get through to him!\"\n",
            "\n",
            "\"Oh, dear!\"  Kate bemoaned, certain the Avon woman had come to call.\n",
            "She brushed her hands over her skirt, straightened her broad shoulders\n",
            "and pushed through to the dining room, determined not to buy a single\n",
            "thing today.\n",
            "\n",
            "\"Good morning, Mrs. Blanchard!\"  beamed the woman in the pale rose\n",
            "colored ensemble. Purse clutched in one hand, sample case in the other, \n",
            "she reminded Kate of the Lady Justice, scales perfectly balanced. But \n",
            "this lady had no blindfold. (All the better to see you with, my dear. \n",
            "And Oh, wouldn't this color just bring on the blush in your cheeks for \n",
            "$11.00 a tube?)  \"Isn't it just a glorious day?\" the woman pronouned, \n",
            "boldly stepping over the threshold on past assumptions.\n",
            "\n",
            "_\"That's it!\"_ Kate thought to herself. She'd let the woman in once,\n",
            "bought gifts soaps and lipstick in the spirit of cooperation, and\n",
            "never been free of past assumptions since. \"Glorious!\" Kate echoed,\n",
            "moving aside before she was trod upon. Rebeccah hovered at the dining\n",
            "room doors. Kate felt her there.\n",
            "\n",
            "\"Oh, and you've brought the day in with you!\" exclaimed the woman,\n",
            "noting cut flowers on mantel and coffee table. \"How healthful!\"\n",
            "\n",
            "\"Healthful?\" Kate inquired.\n",
            "\n",
            "\"Oh, yes. Studies have shown that people who surround themselves with\n",
            "live plants and fresh flowers indoors live longer, feel better, and\n",
            "enjoy life more fully.\"\n",
            "\n",
            "\"Coffee?\"  Kate offered as the woman sat on the edge of the sofa. It\n",
            "was the one torment she allowed herself to use on the woman, knowing\n",
            "full well this door to door saleswoman would shun other people's\n",
            "bathrooms.\n",
            "\n",
            "\"No thank you,\" she answered, a slight grimace flashing across her\n",
            "face as she scooted forward and opened her case.\n",
            "\n",
            "\"You're so rude!\" Rebeccah crowed, having come closer. \"She's got a\n",
            "bladder full now.\"\n",
            "\n",
            "Kate smiled, holding back a giggle. She was certain she'd scored\n",
            "without knowing why. The woman drew forth brightly colored sheets of\n",
            "paper and placed them neatly before Kate on the glass topped table.\n",
            "_\"A promotional,\"_ Kate moaned within her mind. At the bottom of each\n",
            "was stamped, in flowing script, \"Eleanor Thomsason.\"  Address and two\n",
            "phone numbers followed in block lettering.\n",
            "\n",
            "\"I don't really need anything today, Eleanor,\" Kate began.\n",
            "\n",
            "\"Of course you don't, dear. You're more than lovely in your house\n",
            "frock and clean scrubbed face. But you must see the new complexion\n",
            "care line we're offering. Designed especially for the woman over 30\n",
            "and her special needs,\" Eleanor pulled full sized display item from\n",
            "the depths of her bottomless case and set them neatly in a row,\n",
            "labels facing the prospective buyer. \"As you can see here,\" she said\n",
            "crisply, long manicured finger nail tapping each item gently as she\n",
            "spoke, \"We have a scrub, toner, tightener, moisturizer and light\n",
            "foundation. The foundation comes in 6 basic colors. Just to smooth\n",
            "over those tiny blotches we all seem to have after 30.\"\n",
            "\n",
            "Kate sat forward in her occasional chair, considering the possibility\n",
            "that she might, indeed, need a little more complexion care. She\n",
            "touched the toner, tilting it slightly to the light. While she was\n",
            "otherwise engaged Eleanor brought forth tubes, bottles and jars of the\n",
            "same line. She busied herself arranging them in a straight line to\n",
            "the left and just behind the first row.\n",
            "\n",
            "\"And here we have the corresponding blush, highlighters, lipsticks \n",
            "and shadows. Now this line is made with completely natural base\n",
            "substances,\" Eleanor pointed out.\n",
            "\n",
            "\"Chemicals,\" Rebeccah commented, coming closer still, intently\n",
            "interested in the ordered presentation.\n",
            "\n",
            "Kate let go the toner and reached for the blush. Eleanor straightened\n",
            "the toner, turning the label toward the prospective buyer. Rebeccah\n",
            "came around the coffee table and sat on the sofa with Eleanor, her\n",
            "arms primly at her sides, hands clasped in her lap. Rebeccah leaned\n",
            "forward in the same manner as did Eleanor.\n",
            "\n",
            "The genial rise and fall of the woman's voice slipped into the background \n",
            "of sounds passing by on the peaceful street outside. Kate blinked once, \n",
            "the blush still clasped within her fingers, watching Eleanor's lips move. \n",
            "She could almost hear Rebeccah.\n",
            "\n",
            "Rebeccah's attention was focused entirely on Eleanor the Avon lady.\n",
            "\"The flowers have been scattered willy-nilly along the walk,\"\n",
            "Rebeccah said conversationally, her lips mere inches from Eleanor's\n",
            "ear. \"They look so untidy.\"  Eleanor looked, suddenly, as if she'd\n",
            "forgotten something. Kate remembered the flour canisters on the\n",
            "bench. \"What we need is someone with some organizational ability,\"\n",
            "Rebeccah continued. \n",
            "\n",
            "Eleanor drew forth her order book. \"Flowers are like life's little \n",
            "markers,\" Rebeccah whispered. Eleanor reached into her case for a \n",
            "marker. \"Yellow markers, as it were, for the days of our lives.\"  \n",
            "Eleanor replaced the fine tipped black marker and retrieved a broad \n",
            "stroke yellow highlighter. Kate seemed to hear McDonald Carey speaking \n",
            "about sand. \"The flowers along the walk NEED straightening.\"\n",
            "\n",
            "\"Will you excuse me, just one moment?\" Kate asked. She knew exactly\n",
            "where to find that hourglass. She rose from her chair\n",
            "\n",
            "\"Certainly, dear,\" Eleanor answered, her mind seemingly elsewhere\n",
            "while her hands compulsively aligned the display items.\n",
            "\n",
            "\"*YOU* could be the only one for the job!\"  Rebeccah spoke\n",
            "authoritatively, her body turned toward Eleanor. \"The flowers need\n",
            "alignment!\"\n",
            "\n",
            "Kate felt an oppressive headache coming on. Two of them in one\n",
            "morning was more than anyone should be expected to bear. As she\n",
            "passed through the kitchen door her spirits seemed to rise suddenly.\n",
            "Sunshine slanted into the room to highlight every gleaming surface,\n",
            "glinting sweetly on glassware and chrome. She inhaled fully, filling\n",
            "her lungs with the aroma of fresh brewed coffee. The hourglass\n",
            "spilling out the days of her life seemed important only in the\n",
            "abstract. All was right today. She thought of the flowers by the\n",
            "walk, then. For some reason she  wanted to see them from the top\n",
            "floor.\n",
            "\n",
            "She poured herself a cup of coffee, carried it up the back stairs to\n",
            "the second floor landing and peered from the window into the side\n",
            "yard. She thought, idly, of the new gardener, and what creative\n",
            "expression he might come up with for that spot there, which had never\n",
            "been cultivated. Onward, to the front of the house, and into the\n",
            "quiet room beneath the pitch of the front eaves. \n",
            "\n",
            "She sat on the window ledge and balanced her cup on the sill, the \n",
            "threatened headache a memory, only, of Saturday afternoons with her \n",
            "mother. Somewhere behind her temples her mother's voice droned on and \n",
            "on; something about book spines and the edge of the shelf. Sometimes \n",
            "one had to learn to ignore the librarian in order to read the books.\n",
            "\n",
            "Her eyes drifted to the front walk. Far below, as if in another\n",
            "world, Eleanor the Avon lady knelt in the grass next to the walk.\n",
            "A tall shadow stood near, softly, insistently coaxing, as Eleanor\n",
            "carefully spaded deep into the earth and removed a daffodil. She\n",
            "placed it gently into a prepared hole, tamped the earth around it and\n",
            "proceeded to dig another hole, exactly six inches from the last, in a\n",
            "perfectly straight line parallel to the walk.\n",
            "\n",
            "\"Oh, for crying out loud!\"  Kate exclaimed, watching closely. \"Those\n",
            "flowers!\"  She'd have to remember to collect the flour canisters\n",
            "before Harold came home. \"Goodness, Rebeccah,\" she continued, with\n",
            "some exasperation, \"why on earth didn't you say `Daffodils'?\"\n",
            "\n",
            "                              # # #\n",
            "\n",
            "Copyright 1994 Gay Bost\n",
            "--------------------------------------------------------------------------\n",
            " Gay is a Clinical Lab Tech with experience in Veterinary medicine. \n",
            "Originally from NORTHERN California, she has resided in Southeast Missouri \n",
            "with her husband and an aggressive 6 year old boy, since 1974. She \n",
            "installed her first modem in the summer of 1992 and has been exploring new \n",
            "worlds since. Her first and only publication, a short horror story, came \n",
            "when she was 17 years old. The success was so overwhelming she called an \n",
            "end to her writing days and went in search of herself. She's still looking. \n",
            "You will find Gay's work in the best Electronic Magazines.\n",
            "===========================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABa9py1C66ZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}